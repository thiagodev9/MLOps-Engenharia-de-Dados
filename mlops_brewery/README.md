# ğŸº MLOps Brewery Project

## ğŸ‡§ğŸ‡· VisÃ£o Geral 

Este projeto foi desenvolvido com o objetivo de **conceitos de Engenharia de Dados e MLOps**, utilizando ferramentas modernas amplamente usadas no mercado.

O pipeline realiza a **ingestÃ£o paginada de dados** da API pÃºblica **Open Brewery**, processa os dados com **PySpark**, armazena os resultados em **formato Parquet** e registra experimentos utilizando **MLflow**. Todo o ambiente Ã© **containerizado com Docker Compose**, garantindo reprodutibilidade e facilidade de execuÃ§Ã£o.

---

## ğŸ‡ºğŸ‡¸ Overview 

This project was created to **Data Engineering and MLOps concepts**.

The pipeline collects data from the public **Open Brewery API**. It uses **pagination** to get all data. The data is processed with **PySpark** and saved in **Parquet format**.

The project uses **Docker Compose**, so it is easy to run on any machine. **MLflow** is used to track experiments and runs.

---

## ğŸ§± Arquitetura do Projeto | Project Architecture

* API REST (Open Brewery)
* PySpark (IngestÃ£o e processamento)
* Parquet (Camada de armazenamento)
* MLflow (Rastreamento de experimentos)
* Docker & Docker Compose (Ambiente)

---

## ğŸ“‚ Estrutura de Pastas | Folder Structure

```
mlops_brewery/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py          # IngestÃ£o paginada com PySpark
â”‚   â”œâ”€â”€ preprocess.py     # Limpeza e preparaÃ§Ã£o dos dados
â”‚   â”œâ”€â”€ train.py          # Treinamento simples de modelo
â”‚   â””â”€â”€ run_pipeline.py   # OrquestraÃ§Ã£o do pipeline
â”œâ”€â”€ data/
â”‚   â””â”€â”€ bronze/            # Dados brutos em Parquet
â””â”€â”€ README.md
```

---

## âš™ï¸ Tecnologias Utilizadas | Technologies Used

* **Python 3.10**
* **PySpark**
* **Docker & Docker Compose**
* **MLflow**
* **Parquet**
* **Open Brewery API**

---

## â–¶ï¸ Como Executar o Projeto | How to Run the Project

### PrÃ©-requisitos | Prerequisites

* Docker
* Docker Compose

### Passos | Steps

```bash
# Clonar o repositÃ³rio
git clone <repo-url>
cd mlops_brewery

# Build e execuÃ§Ã£o
docker compose build --no-cache
docker compose up
```

ApÃ³s a execuÃ§Ã£o:

* O pipeline serÃ¡ executado automaticamente
* Os dados serÃ£o salvos em Parquet
* O MLflow ficarÃ¡ disponÃ­vel em:

ğŸ‘‰ [http://localhost:5000](http://localhost:5000)

---

## ğŸ§  Principais Aprendizados | Key Learnings

### ğŸ‡§ğŸ‡· PortuguÃªs

* IngestÃ£o paginada de APIs REST
* Uso do PySpark para processamento distribuÃ­do
* ResoluÃ§Ã£o de problemas de schema inconsistente
* PersistÃªncia de dados em Parquet
* ContainerizaÃ§Ã£o de pipelines de dados
* Uso do MLflow para rastreamento de experimentos

### ğŸ‡ºğŸ‡¸ English (Simple A2)

During data ingestion, PySpark had an error because some fields had different data types.

To fix this problem, the schema was defined manually. This made the ingestion process stable and correct.

The Java version was also updated to **OpenJDK 21** to work correctly with Spark.
